{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install qwen_vl_utils bitsandbytes accelerate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6cB8aLKuBhPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5Lgg8BmwBh6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2K3XnIvb8gcS"
      },
      "outputs": [],
      "source": [
        "# 1. Install aria2 (a multi-source download utility)\n",
        "!apt-get install -y -qq aria2\n",
        "\n",
        "# 2. Download AZ.zip using 16 connections\n",
        "# -x 16: use 16 connections\n",
        "# -s 16: split file into 16 parts\n",
        "# -o: output filename\n",
        "print(\"Starting optimized download...\")\n",
        "!aria2c -x 16 -s 16 -o AZ.zip \"https://zenodo.org/records/13852757/files/AZ.zip?download=1\"\n",
        "\n",
        "# 3. Unzip as before\n",
        "print(\"Download complete. Extracting...\")\n",
        "!unzip -q AZ.zip -d ./IdNet_Data\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5WjdBr5JD-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, random\n",
        "\n",
        "# Create 'images' directory\n",
        "os.makedirs('images', exist_ok=True)\n",
        "print(\"Created 'images' directory.\")\n",
        "\n",
        "# Create 'labels' directory\n",
        "os.makedirs('labels', exist_ok=True)\n",
        "print(\"Created 'labels' directory.\")\n",
        "\n",
        "source_dir = '/content/IdNet_Data/AZ/positive'\n",
        "dest_dir = 'images'\n",
        "\n",
        "# Ensure the destination directory exists (already created in previous step, but good practice)\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Get a list of all files in the source directory\n",
        "files_to_copy = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
        "\n",
        "print(f\"Copying {len(files_to_copy)} files from '{source_dir}' to '{dest_dir}'...\")\n",
        "\n",
        "for filename in files_to_copy:\n",
        "    source_path = os.path.join(source_dir, filename)\n",
        "    destination_path = os.path.join(dest_dir, filename)\n",
        "    shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(\"Finished copying files to 'images' directory.\")\n",
        "\n",
        "\n",
        "source_dir = '/content/IdNet_Data/AZ/meta/basic'\n",
        "dest_dir = 'labels'\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Get a list of all files in the source directory\n",
        "files_to_copy = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
        "\n",
        "print(f\"Copying {len(files_to_copy)} files from '{source_dir}' to '{dest_dir}'...\")\n",
        "\n",
        "for filename in files_to_copy:\n",
        "    source_path = os.path.join(source_dir, filename)\n",
        "    destination_path = os.path.join(dest_dir, filename)\n",
        "    shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(\"Finished copying files to 'labels' directory.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Directory to process\n",
        "target_dir = 'labels'\n",
        "\n",
        "# Mapping from Source Key -> Target Key\n",
        "key_mapping = {\n",
        "    \"first_name\": \"first_name\",\n",
        "    \"last_name\": \"last_name\",\n",
        "    \"address\": \"address\",\n",
        "    \"birthday\": \"DOB\",\n",
        "    \"gender\": \"SEX\",\n",
        "    \"class\": \"CLASS\",\n",
        "    \"issue_date\": \"ISS\",\n",
        "    \"expire_date\": \"EXP\",\n",
        "    \"height\": \"HGT\",\n",
        "    \"weight\": \"WGT\",\n",
        "    \"eye_color\": \"EYES\",\n",
        "    \"license_number\": \"DLN\"\n",
        "}\n",
        "\n",
        "print(f\"Processing files in '{target_dir}' to rename fields and match template...\")\n",
        "\n",
        "files = [f for f in os.listdir(target_dir) if f.endswith('.json')]\n",
        "count = 0\n",
        "\n",
        "for filename in files:\n",
        "    path = os.path.join(target_dir, filename)\n",
        "    try:\n",
        "        with open(path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # 1. Handle Name splitting if source has 'name' but not 'first_name'\n",
        "        if \"first_name\" not in data and \"name\" in data:\n",
        "            parts = data['name'].split(' ', 1)\n",
        "            data['first_name'] = parts[0]\n",
        "            data['last_name'] = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "        # 2. Create new dictionary with only the target keys\n",
        "        new_data = {}\n",
        "        for source_key, target_key in key_mapping.items():\n",
        "            # Use the mapped source key if it exists, otherwise mapping might mean\n",
        "            # the key is the same (e.g. first_name -> first_name)\n",
        "            # Ideally we look for the source_key in data.\n",
        "\n",
        "            # Check if we need to look up by the 'old' name corresponding to the new name\n",
        "            # The mapping above is explicit: source_key_name -> target_key_name\n",
        "            # So we look up 'source_key' in 'data'.\n",
        "\n",
        "            if source_key in data:\n",
        "                new_data[target_key] = data[source_key]\n",
        "            elif target_key in data:\n",
        "                # Fallback: if the data already has the target key (e.g. first_name)\n",
        "                new_data[target_key] = data[target_key]\n",
        "            else:\n",
        "                new_data[target_key] = None # or \"\" or skip\n",
        "\n",
        "        # 3. Overwrite the file\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(new_data, f, indent=2)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "print(f\"Successfully updated {count} label files.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create 'ho_img' directory for holdout images\n",
        "os.makedirs('ho_img', exist_ok=True)\n",
        "print(\"Created 'ho_img' directory.\")\n",
        "\n",
        "# Create 'ho_label' directory for holdout labels\n",
        "os.makedirs('ho_label', exist_ok=True)\n",
        "print(\"Created 'ho_label' directory.\")\n",
        "\n",
        "image_dir = 'images'\n",
        "label_dir = 'labels'\n",
        "ho_img_dir = 'ho_img'\n",
        "ho_label_dir = 'ho_label'\n",
        "\n",
        "# Get all image files\n",
        "all_image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "\n",
        "# Calculate 10% for holdout\n",
        "holdout_percentage = 0.10\n",
        "num_holdout_files = int(len(all_image_files) * holdout_percentage)\n",
        "\n",
        "# Randomly select files for holdout\n",
        "random.seed(42) # for reproducibility\n",
        "holdout_image_files = random.sample(all_image_files, num_holdout_files)\n",
        "\n",
        "print(f\"Total images: {len(all_image_files)}\")\n",
        "print(f\"Number of holdout files to move: {num_holdout_files}\")\n",
        "\n",
        "# Move selected files and their corresponding labels\n",
        "moved_count = 0\n",
        "for img_filename in holdout_image_files:\n",
        "    # Determine corresponding label filename (assuming same base name, different extension)\n",
        "    base_name, _ = os.path.splitext(img_filename)\n",
        "    label_filename = f\"{base_name}.json\" # Assuming labels are JSON files\n",
        "\n",
        "    source_img_path = os.path.join(image_dir, img_filename)\n",
        "    dest_img_path = os.path.join(ho_img_dir, img_filename)\n",
        "\n",
        "    source_label_path = os.path.join(label_dir, label_filename)\n",
        "    dest_label_path = os.path.join(ho_label_dir, label_filename)\n",
        "\n",
        "    # Move image file\n",
        "    if os.path.exists(source_img_path):\n",
        "        shutil.move(source_img_path, dest_img_path)\n",
        "        moved_count += 1\n",
        "    else:\n",
        "        print(f\"Warning: Image file not found: {source_img_path}\")\n",
        "\n",
        "    # Move corresponding label file\n",
        "    if os.path.exists(source_label_path):\n",
        "        shutil.move(source_label_path, dest_label_path)\n",
        "    else:\n",
        "        print(f\"Warning: Label file not found: {source_label_path} for image {img_filename}\")\n",
        "\n",
        "print(f\"Moved {moved_count} image-label pairs to holdout directories.\")"
      ],
      "metadata": {
        "id": "jct4kbKDBfw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoConfig\n",
        "\n",
        "# Robust import for the model class handling potential version discrepancies\n",
        "try:\n",
        "    from transformers import AutoModelForVision2Seq\n",
        "except ImportError:\n",
        "    print(\"AutoModelForVision2Seq not found. Falling back to AutoModel.\")\n",
        "    from transformers import AutoModel as AutoModelForVision2Seq\n",
        "\n",
        "model_name = \"numind/NuExtract-2.0-4B\"\n",
        "\n",
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU detected. Loading model in bfloat16 (optimized for L4)...\")\n",
        "    # L4 supports bfloat16 and Flash Attention 2\n",
        "    # 4B params in bfloat16 = ~8GB VRAM. L4 has 22.5GB, so this fits comfortably.\n",
        "    model = AutoModelForVision2Seq.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        #device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"flash_attention_2\" # Enable Flash Attention 2 for L4\n",
        "    )\n",
        "else:\n",
        "    print(\"No GPU detected. Loading model in CPU mode (float32).\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Loading processor for {model_name}...\")\n",
        "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "print(\"Model and Processor loaded successfully.\")"
      ],
      "metadata": {
        "id": "Yo6vndvvHITM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from dateutil import parser  # For smart date comparison\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
        "\n",
        "# Ensure qwen-vl-utils is installed\n",
        "try:\n",
        "    from qwen_vl_utils import process_vision_info\n",
        "except ImportError:\n",
        "    print(\"Installing qwen-vl-utils...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"qwen-vl-utils\"])\n",
        "    from qwen_vl_utils import process_vision_info\n",
        "\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "\n",
        "# Robust import logic for the Model Class\n",
        "try:\n",
        "    from transformers import Qwen2_5_VLForConditionalGeneration\n",
        "    ModelClass = Qwen2_5_VLForConditionalGeneration\n",
        "except ImportError:\n",
        "    try:\n",
        "        from transformers import AutoModelForVision2Seq\n",
        "        ModelClass = AutoModelForVision2Seq\n",
        "    except ImportError:\n",
        "        ModelClass = AutoModelForCausalLM\n",
        "\n",
        "model_name = \"numind/NuExtract-2.0-4B\"\n",
        "\n",
        "# Load Model\n",
        "print(f\"Loading model: {model_name}...\")\n",
        "\n",
        "# FORCE GPU LOADING\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU detected. Loading model explicitly on CUDA device...\")\n",
        "    model = ModelClass.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"flash_attention_2\"\n",
        "    ).to(\"cuda\")\n",
        "else:\n",
        "    model = ModelClass.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"cpu\",\n",
        "        torch_dtype=torch.float32,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# --- UPDATED TEMPLATE: Matching Ground Truth Schema ---\n",
        "template = json.dumps({\n",
        "    \"first_name\": \"verbatim-string\",\n",
        "    \"last_name\": \"verbatim-string\",\n",
        "    \"address\": \"verbatim-string\",\n",
        "    \"DOB\": \"verbatim-string\",\n",
        "    \"SEX\": \"verbatim-string\",\n",
        "    \"CLASS\": \"verbatim-string\",\n",
        "    \"ISS\": \"verbatim-string\",\n",
        "    \"EXP\": \"verbatim-string\",\n",
        "    \"HGT\": \"verbatim-string\",\n",
        "    \"WGT\": \"verbatim-string\",\n",
        "    \"EYES\": \"verbatim-string\",\n",
        "    \"DLN\": \"verbatim-string\"\n",
        "})\n",
        "\n",
        "# Inference Function\n",
        "def predict_nu_extract(image_path, model, processor, template):\n",
        "    prompt_text = f\"# Template:\\n{template}\\n# Context:\\n<|vision_start|><|image_pad|><|vision_end|>\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image_path},\n",
        "                {\"type\": \"text\", \"text\": prompt_text},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "\n",
        "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
        "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    return output_text\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Lowercases and removes punctuation for fairer comparison.\"\"\"\n",
        "    if not text: return \"\"\n",
        "    # Remove non-alphanumeric characters (keep spaces)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text.lower().strip()\n",
        "\n",
        "def normalize_date(text):\n",
        "    \"\"\"Parses dates to allow YYYY-MM-DD to match MM/DD/YYYY.\"\"\"\n",
        "    if not text: return \"\"\n",
        "    try:\n",
        "        # Parse string to datetime (handles most formats automatically)\n",
        "        dt = parser.parse(str(text), fuzzy=True)\n",
        "        # Return as standard YYYY-MM-DD string\n",
        "        return dt.strftime(\"%Y-%m-%d\")\n",
        "    except:\n",
        "        # If parsing fails, fall back to text normalization\n",
        "        return normalize_text(str(text))\n",
        "\n",
        "# Evaluation Logic\n",
        "ho_img_dir = \"ho_img\"\n",
        "ho_label_dir = \"ho_label\"\n",
        "log_filename = \"evaluation_log_exp.txt\"\n",
        "\n",
        "# Initialize Metrics\n",
        "template_keys = list(json.loads(template).keys())\n",
        "field_stats = {key: {'correct': 0, 'total': 0} for key in template_keys}\n",
        "\n",
        "image_files = [f for f in os.listdir(ho_img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# --- Run on full holdout set ---\n",
        "subset_size = 597\n",
        "image_files_subset = image_files[:subset_size]\n",
        "\n",
        "print(f\"Starting evaluation on {len(image_files_subset)} images...\\n\")\n",
        "\n",
        "with open(log_filename, \"w\") as log_file:\n",
        "    log_file.write(\"EVALUATION DETAILED LOG (Smart Date & Text Comparison)\\n\")\n",
        "    log_file.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "    for img_filename in tqdm(image_files_subset):\n",
        "        img_path = os.path.join(ho_img_dir, img_filename)\n",
        "        label_filename = os.path.splitext(img_filename)[0] + \".json\"\n",
        "        label_path = os.path.join(ho_label_dir, label_filename)\n",
        "\n",
        "        if not os.path.exists(label_path):\n",
        "            continue\n",
        "\n",
        "        # Load Ground Truth\n",
        "        with open(label_path, 'r') as f:\n",
        "            gt_data = json.load(f)\n",
        "\n",
        "        # Run Prediction\n",
        "        try:\n",
        "            prediction_str = predict_nu_extract(img_path, model, processor, template)\n",
        "            pred_data = json.loads(prediction_str)\n",
        "        except Exception as e:\n",
        "            pred_data = {}\n",
        "\n",
        "        # Write details to log\n",
        "        log_file.write(f\"--- Image: {img_filename} ---\\n\")\n",
        "        log_file.write(\"PREDICTED:\\n\")\n",
        "        log_file.write(json.dumps(pred_data, indent=2) + \"\\n\")\n",
        "        log_file.write(\"GROUND TRUTH:\\n\")\n",
        "        log_file.write(json.dumps(gt_data, indent=2) + \"\\n\")\n",
        "\n",
        "        # Compare Fields\n",
        "        mismatches = []\n",
        "        for key in template_keys:\n",
        "            gt_val = gt_data.get(key, None)\n",
        "            pred_val = pred_data.get(key, None)\n",
        "\n",
        "            # Convert to strings if they exist, else empty string\n",
        "            gt_str = str(gt_val) if gt_val is not None else \"\"\n",
        "            pred_str = str(pred_val) if pred_val is not None else \"\"\n",
        "\n",
        "            # Use Date Normalization for date fields, Text for others\n",
        "            if key in [\"DOB\", \"ISS\", \"EXP\"]:\n",
        "                gt_norm = normalize_date(gt_str)\n",
        "                pred_norm = normalize_date(pred_str)\n",
        "            else:\n",
        "                gt_norm = normalize_text(gt_str)\n",
        "                pred_norm = normalize_text(pred_str)\n",
        "\n",
        "            # Update Stats\n",
        "            if gt_norm == pred_norm:\n",
        "                field_stats[key]['correct'] += 1\n",
        "            else:\n",
        "                mismatches.append(f\"{key}: '{gt_str}' != '{pred_str}'\")\n",
        "\n",
        "            field_stats[key]['total'] += 1\n",
        "\n",
        "        if mismatches:\n",
        "            log_file.write(\"MISMATCHES:\\n\" + \"\\n\".join(mismatches) + \"\\n\")\n",
        "        else:\n",
        "            log_file.write(\"RESULT: Perfect Match\\n\")\n",
        "\n",
        "        log_file.write(\"\\n\" + \"-\"*40 + \"\\n\\n\")\n",
        "\n",
        "    # --- Generate Report ---\n",
        "    report_lines = []\n",
        "    report_lines.append(\"\\n\" + \"=\"*40)\n",
        "    report_lines.append(f\"EVALUATION RESULTS (Subset: {subset_size} images)\")\n",
        "    report_lines.append(\"=\"*40)\n",
        "\n",
        "    report_lines.append(f\"{ 'FIELD':<20} | { 'ACCURACY':<10} | { 'CORRECT':<8} / { 'TOTAL'}\")\n",
        "    report_lines.append(\"-\"*50)\n",
        "\n",
        "    for key in template_keys:\n",
        "        stats = field_stats[key]\n",
        "        acc = (stats['correct'] / stats['total']) * 100 if stats['total'] > 0 else 0.0\n",
        "        report_lines.append(f\"{key:<20} | {acc:>9.2f}% | {stats['correct']:<8} / {stats['total']}\")\n",
        "\n",
        "    report_lines.append(\"-\"*50)\n",
        "    final_report = \"\\n\".join(report_lines)\n",
        "\n",
        "    print(final_report)\n",
        "    log_file.write(final_report)\n",
        "\n",
        "print(f\"\\nLog saved to {log_filename}\")"
      ],
      "metadata": {
        "id": "u8vndGjFHZs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "log_file_path = '/content/evaluation_log_exp.txt'\n",
        "\n",
        "total_correct = 0\n",
        "total_count = 0\n",
        "\n",
        "try:\n",
        "    with open(log_file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    print(\"Parsing evaluation log for total accuracy...\")\n",
        "\n",
        "    # We look for lines in the summary table that contain the counts\n",
        "    # Format: Field | Accuracy | Correct / Total\n",
        "    # Regex to capture the \"Correct / Total\" part at the end of the line\n",
        "    # e.g. \"| 551      / 597\"\n",
        "    pattern = re.compile(r'\\|\\s+(\\d+)\\s+/\\s+(\\d+)\\s*$')\n",
        "\n",
        "    for line in lines:\n",
        "        match = pattern.search(line)\n",
        "        if match:\n",
        "            correct = int(match.group(1))\n",
        "            total = int(match.group(2))\n",
        "            total_correct += correct\n",
        "            total_count += total\n",
        "\n",
        "    if total_count > 0:\n",
        "        overall_accuracy = (total_correct / total_count) * 100\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"TOTAL AGGREGATE ACCURACY\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"Total Fields:  {total_count}\")\n",
        "        print(f\"Total Correct: {total_correct}\")\n",
        "        print(f\"Accuracy:      {overall_accuracy:.2f}%\")\n",
        "        print(\"=\" * 40)\n",
        "    else:\n",
        "        print(\"No accuracy data found in the log file.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{log_file_path}' not found. Please ensure the evaluation step completed.\")"
      ],
      "metadata": {
        "id": "E9Vrb36jt6do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U trl peft"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ibd1MXxgBIiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- VERSION CHECK & ROBUST IMPORT ---\n",
        "try:\n",
        "    from transformers import AutoModelForVision2Seq\n",
        "except ImportError:\n",
        "    print(\"⚠️ AutoModelForVision2Seq not found. Falling back to AutoModelForImageTextToText...\")\n",
        "    from transformers import AutoModelForImageTextToText as AutoModelForVision2Seq\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"numind/NuExtract-2.0-4B\"\n",
        "DATA_DIR = \"./labels\"\n",
        "IMAGE_DIR = \"./images\"\n",
        "OUTPUT_DIR = \"./nuextract_id_finetune\"\n",
        "\n",
        "# --- 1. Load Model & Processor ---\n",
        "print(\"Loading Model...\")\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    device_map=\"auto\",\n",
        "    # use_cache=False,  <-- REMOVED: Caused the TypeError\n",
        "    quantization_config={\"load_in_4bit\": True, \"bnb_4bit_compute_dtype\": torch.bfloat16}\n",
        ")\n",
        "\n",
        "# Disable cache in config instead\n",
        "model.config.use_cache = False\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True,\n",
        "    padding_side='right',\n",
        "    use_fast=True,\n",
        ")\n",
        "\n",
        "# Apply LoRA Adapter\n",
        "peft_config = LoraConfig(\n",
        "    r=16, lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    bias=\"none\"\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- 2. Message Formatting ---\n",
        "def construct_messages(image_path, template, label_str):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # NuExtract 2.0 specific placeholder format\n",
        "    image_placeholder = \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
        "    text_content = f\"# Template:\\n{template}\\n# Context:\\n{image_placeholder}\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": text_content},\n",
        "                {\"type\": \"image\", \"image\": image}\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": label_str}],\n",
        "        }\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "# --- 3. Prepare Dataset ---\n",
        "print(\"Scanning Data...\")\n",
        "id_card_template = json.dumps({\n",
        "    \"first_name\": \"verbatim-string\", \"last_name\": \"verbatim-string\",\n",
        "    \"address\": \"verbatim-string\", \"DOB\": \"verbatim-string\",\n",
        "    \"SEX\": \"verbatim-string\", \"CLASS\": \"verbatim-string\",\n",
        "    \"ISS\": \"verbatim-string\", \"EXP\": \"verbatim-string\",\n",
        "    \"HGT\": \"verbatim-string\", \"WGT\": \"verbatim-string\",\n",
        "    \"EYES\": \"verbatim-string\", \"DLN\": \"verbatim-string\"\n",
        "}, indent=None)\n",
        "\n",
        "formatted_dataset = []\n",
        "files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".json\")]\n",
        "\n",
        "for json_file in files:\n",
        "    base_name = os.path.splitext(json_file)[0]\n",
        "    json_path = os.path.join(DATA_DIR, json_file)\n",
        "    # Find Image\n",
        "    for ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
        "        possible = os.path.join(IMAGE_DIR, base_name + ext)\n",
        "        if os.path.exists(possible):\n",
        "            try:\n",
        "                with open(json_path, 'r') as f:\n",
        "                    target_str = json.dumps(json.load(f), indent=None)\n",
        "                msg = construct_messages(os.path.abspath(possible), id_card_template, target_str)\n",
        "                formatted_dataset.append(msg)\n",
        "            except: pass\n",
        "            break\n",
        "\n",
        "print(f\"Loaded {len(formatted_dataset)} valid samples.\")\n",
        "\n",
        "# --- 4. Collator ---\n",
        "def collate_fn(examples):\n",
        "    user_texts = [processor.apply_chat_template(x[:1], tokenize=False) for x in examples]\n",
        "    full_texts = [processor.apply_chat_template(x, tokenize=False) for x in examples]\n",
        "    image_inputs = process_vision_info(examples)[0]\n",
        "\n",
        "    # Batch Tokenization\n",
        "    user_batch = processor(text=user_texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
        "    full_batch = processor(text=full_texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Create Labels with Masking\n",
        "    labels = full_batch[\"input_ids\"].clone()\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "    for i in range(len(examples)):\n",
        "        user_len = user_batch[\"attention_mask\"][i].sum().item()\n",
        "        labels[i, :user_len - 1] = -100 # Mask user prompt\n",
        "\n",
        "    full_batch[\"labels\"] = labels\n",
        "    return full_batch\n",
        "\n",
        "# --- 5. Start Training ---\n",
        "print(\"Starting Training...\")\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2, # L4 Optimized\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=formatted_dataset,\n",
        "    processing_class=processor.tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "processor.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Training Complete. Model saved.\")"
      ],
      "metadata": {
        "id": "99rv1nf6xQmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# 1. Delete the specific objects holding memory\n",
        "# (Add any other variable names you defined in your previous run)\n",
        "try:\n",
        "    del model\n",
        "    del trainer\n",
        "    del inputs\n",
        "    del processor\n",
        "except NameError:\n",
        "    pass # Variables might not exist, which is fine\n",
        "\n",
        "# 2. Force Python's Garbage Collector to release memory\n",
        "gc.collect()\n",
        "\n",
        "# 3. Clear PyTorch's internal cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 4. Verify memory is cleared\n",
        "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "print(f\"Reserved:  {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "id": "KzfXsJxLHqhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from dateutil import parser\n",
        "\n",
        "# --- 1. Imports (Robust) ---\n",
        "try:\n",
        "    from transformers import AutoModelForVision2Seq\n",
        "except ImportError:\n",
        "    from transformers import AutoModelForImageTextToText as AutoModelForVision2Seq\n",
        "\n",
        "from transformers import AutoProcessor\n",
        "from peft import PeftModel\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "# Only need to reload paths if variables were lost, but safe to redefine\n",
        "BASE_MODEL_ID = \"numind/NuExtract-2.0-4B\"\n",
        "ADAPTER_PATH = \"./nuextract_id_finetune\"\n",
        "HO_IMG_DIR = \"ho_img\"\n",
        "HO_LABEL_DIR = \"ho_label\"\n",
        "BATCH_SIZE = 16  # FAST BATCH PROCESSING\n",
        "\n",
        "# --- 3. Load Model (Only if not already loaded) ---\n",
        "if 'model' not in globals():\n",
        "    print(\"Loading Model & Adapter...\")\n",
        "    model = AutoModelForVision2Seq.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        "        device_map=\"auto\",\n",
        "        quantization_config={\"load_in_4bit\": True, \"bnb_4bit_compute_dtype\": torch.bfloat16}\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
        "    model.eval()\n",
        "    processor = AutoProcessor.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
        "else:\n",
        "    print(\"Model already loaded. Skipping reload.\")\n",
        "\n",
        "print(f\"Using Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "# --- 4. Batched Inference Function ---\n",
        "def batch_predict(image_paths, model, processor, template):\n",
        "    batch_messages = []\n",
        "    image_placeholder = \"<|vision_start|><|image_pad|><|vision_end|>\"\n",
        "    prompt_text = f\"# Template:\\n{template}\\n# Context:\\n{image_placeholder}\"\n",
        "\n",
        "    # Load batch of images\n",
        "    loaded_images = [Image.open(p).convert(\"RGB\") for p in image_paths]\n",
        "\n",
        "    for img in loaded_images:\n",
        "        msg = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt_text},\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "        batch_messages.append(msg)\n",
        "\n",
        "    # Process inputs\n",
        "    texts = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]\n",
        "    image_inputs, video_inputs = process_vision_info(batch_messages)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=texts,\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "\n",
        "    # Decode\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    output_texts = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "\n",
        "    return output_texts\n",
        "\n",
        "# --- 5. Helpers ---\n",
        "def normalize_text(text):\n",
        "    if not text: return \"\"\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text.lower().strip()\n",
        "\n",
        "def normalize_date(text):\n",
        "    if not text: return \"\"\n",
        "    try:\n",
        "        dt = parser.parse(str(text), fuzzy=True)\n",
        "        return dt.strftime(\"%Y-%m-%d\")\n",
        "    except:\n",
        "        return normalize_text(str(text))\n",
        "\n",
        "# --- 6. Main Evaluation Loop (Detailed Logging) ---\n",
        "template = json.dumps({\n",
        "    \"first_name\": \"verbatim-string\", \"last_name\": \"verbatim-string\",\n",
        "    \"address\": \"verbatim-string\", \"DOB\": \"verbatim-string\",\n",
        "    \"SEX\": \"verbatim-string\", \"CLASS\": \"verbatim-string\",\n",
        "    \"ISS\": \"verbatim-string\", \"EXP\": \"verbatim-string\",\n",
        "    \"HGT\": \"verbatim-string\", \"WGT\": \"verbatim-string\",\n",
        "    \"EYES\": \"verbatim-string\", \"DLN\": \"verbatim-string\"\n",
        "})\n",
        "\n",
        "template_keys = list(json.loads(template).keys())\n",
        "field_stats = {key: {'correct': 0, 'total': 0} for key in template_keys}\n",
        "image_files = [f for f in os.listdir(HO_IMG_DIR) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Create output log\n",
        "log_filename = \"finetuned_eval_log_detailed.txt\"\n",
        "with open(log_filename, \"w\") as log_file:\n",
        "    log_file.write(f\"EVALUATION DETAILED LOG (Batch Size {BATCH_SIZE})\\n\" + \"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "# Process ALL images\n",
        "subset = image_files\n",
        "batches = [subset[i:i + BATCH_SIZE] for i in range(0, len(subset), BATCH_SIZE)]\n",
        "\n",
        "print(f\"Starting evaluation on {len(subset)} images ({len(batches)} batches)...\")\n",
        "\n",
        "for batch_files in tqdm(batches):\n",
        "    batch_img_paths = []\n",
        "    batch_gt_data = []\n",
        "    valid_files = []\n",
        "\n",
        "    # Pre-load labels to ensure we only process valid pairs\n",
        "    for img_filename in batch_files:\n",
        "        label_filename = os.path.splitext(img_filename)[0] + \".json\"\n",
        "        label_path = os.path.join(HO_LABEL_DIR, label_filename)\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                batch_gt_data.append(json.load(f))\n",
        "            batch_img_paths.append(os.path.join(HO_IMG_DIR, img_filename))\n",
        "            valid_files.append(img_filename)\n",
        "\n",
        "    if not batch_img_paths:\n",
        "        continue\n",
        "\n",
        "    # Run Batch\n",
        "    try:\n",
        "        batch_predictions = batch_predict(batch_img_paths, model, processor, template)\n",
        "    except Exception as e:\n",
        "        print(f\"Batch Failed: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Process & Log Results\n",
        "    for i, pred_str in enumerate(batch_predictions):\n",
        "        img_name = valid_files[i]\n",
        "        gt_data = batch_gt_data[i]\n",
        "\n",
        "        try:\n",
        "            pred_data = json.loads(pred_str)\n",
        "        except:\n",
        "            pred_data = {}\n",
        "\n",
        "        # --- LOGGING TO MATCH YOUR PREFERRED FORMAT ---\n",
        "        with open(log_filename, \"a\") as log_file:\n",
        "            log_file.write(f\"--- Image: {img_name} ---\\n\")\n",
        "            log_file.write(\"PREDICTED:\\n\")\n",
        "            log_file.write(json.dumps(pred_data, indent=2) + \"\\n\")\n",
        "            log_file.write(\"GROUND TRUTH:\\n\")\n",
        "            log_file.write(json.dumps(gt_data, indent=2) + \"\\n\")\n",
        "\n",
        "            mismatches = []\n",
        "            for key in template_keys:\n",
        "                gt_val = str(gt_data.get(key, \"\"))\n",
        "                pred_val = str(pred_data.get(key, \"\"))\n",
        "\n",
        "                # Smart Comparison\n",
        "                if key in [\"DOB\", \"ISS\", \"EXP\"]:\n",
        "                    match = normalize_date(gt_val) == normalize_date(pred_val)\n",
        "                else:\n",
        "                    match = normalize_text(gt_val) == normalize_text(pred_val)\n",
        "\n",
        "                if match:\n",
        "                    field_stats[key]['correct'] += 1\n",
        "                else:\n",
        "                    mismatches.append(f\"{key}: '{gt_val}' != '{pred_val}'\")\n",
        "                field_stats[key]['total'] += 1\n",
        "\n",
        "            if mismatches:\n",
        "                log_file.write(\"MISMATCHES:\\n\" + \"\\n\".join(mismatches) + \"\\n\")\n",
        "            else:\n",
        "                log_file.write(\"RESULT: Perfect Match\\n\")\n",
        "\n",
        "            log_file.write(\"\\n\" + \"-\"*40 + \"\\n\\n\")\n",
        "\n",
        "# Final Report\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"EVALUATION RESULTS (Total: {len(subset)} images)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"{ 'FIELD':<20} | { 'ACCURACY':<10} | { 'CORRECT':<8} / { 'TOTAL'}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "with open(log_filename, \"a\") as log_file:\n",
        "    log_file.write(\"\\n\" + \"=\"*50 + \"\\nEVALUATION RESULTS\\n\" + \"=\"*50 + \"\\n\")\n",
        "    log_file.write(f\"{ 'FIELD':<20} | { 'ACCURACY':<10} | { 'CORRECT':<8} / { 'TOTAL'}\\n\")\n",
        "    log_file.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "    for key in template_keys:\n",
        "        stats = field_stats[key]\n",
        "        acc = (stats['correct'] / stats['total']) * 100 if stats['total'] > 0 else 0.0\n",
        "\n",
        "        # Build the line\n",
        "        line = f\"{key:<20} | {acc:>9.2f}% | {stats['correct']:<8} / {stats['total']}\"\n",
        "\n",
        "        print(line)\n",
        "        log_file.write(line + \"\\n\")\n",
        "\n",
        "    log_file.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "print(f\"\\nDetailed log saved to {log_filename}\")"
      ],
      "metadata": {
        "id": "7o3Mt3tHOtXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Zip the folder\n",
        "# This creates 'nuextract_id_finetune.zip' from your folder\n",
        "print(\"Zipping model folder... (This may take a minute)\")\n",
        "shutil.make_archive(\"/content/nuextract_id_finetune\", 'zip', \"/content/nuextract_id_finetune\")\n",
        "print(\"Zip created!\")\n",
        "\n",
        "# 2. Trigger Download\n",
        "print(\"Downloading...\")\n",
        "files.download(\"/content/nuextract_id_finetune.zip\")"
      ],
      "metadata": {
        "id": "HtHAOkBwYGTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "log_file_path = '/content/finetuned_eval_log_detailed.txt'\n",
        "\n",
        "total_correct = 0\n",
        "total_count = 0\n",
        "\n",
        "try:\n",
        "    with open(log_file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    print(\"Parsing evaluation log for total accuracy...\")\n",
        "\n",
        "    # We look for lines in the summary table that contain the counts\n",
        "    # Format: Field | Accuracy | Correct / Total\n",
        "    # Regex to capture the \"Correct / Total\" part at the end of the line\n",
        "    # e.g. \"| 551      / 597\"\n",
        "    pattern = re.compile(r'\\|\\s+(\\d+)\\s+/\\s+(\\d+)\\s*$')\n",
        "\n",
        "    for line in lines:\n",
        "        match = pattern.search(line)\n",
        "        if match:\n",
        "            correct = int(match.group(1))\n",
        "            total = int(match.group(2))\n",
        "            total_correct += correct\n",
        "            total_count += total\n",
        "\n",
        "    if total_count > 0:\n",
        "        overall_accuracy = (total_correct / total_count) * 100\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"TOTAL AGGREGATE ACCURACY\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"Total Fields:  {total_count}\")\n",
        "        print(f\"Total Correct: {total_correct}\")\n",
        "        print(f\"Accuracy:      {overall_accuracy:.2f}%\")\n",
        "        print(\"=\" * 40)\n",
        "    else:\n",
        "        print(\"No accuracy data found in the log file.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{log_file_path}' not found. Please ensure the evaluation step completed.\")"
      ],
      "metadata": {
        "id": "hRqxYIbgOIHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import re\n",
        "\n",
        "def plot_mismatches(log_file_path, image_dir, max_plots=10):\n",
        "    \"\"\"\n",
        "    Parses the evaluation log and plots images that had extraction errors.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(log_file_path):\n",
        "        print(f\"Error: Log file not found at {log_file_path}\")\n",
        "        return\n",
        "\n",
        "    with open(log_file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Split the log by image entries\n",
        "    # The regex looks for the separator \"--- Image: filename ---\"\n",
        "    entries = re.split(r'--- Image: (.*?) ---\\n', content)[1:]\n",
        "\n",
        "    plot_count = 0\n",
        "\n",
        "    # Iterate through entries (filename is at index i, details at i+1)\n",
        "    for i in range(0, len(entries), 2):\n",
        "        if plot_count >= max_plots:\n",
        "            break\n",
        "\n",
        "        filename = entries[i].strip()\n",
        "        details = entries[i+1]\n",
        "\n",
        "        # We only care about entries with MISMATCHES\n",
        "        if \"MISMATCHES:\" in details:\n",
        "            # Extract the mismatch section\n",
        "            mismatch_section = details.split(\"MISMATCHES:\")[1].split(\"RESULT:\")[0].strip()\n",
        "            # Also extract the prediction for context if needed, or just show the diff\n",
        "            # Let's clean up the mismatch text for display\n",
        "            lines = [line.strip() for line in mismatch_section.split('\\n') if line.strip()]\n",
        "\n",
        "            img_path = os.path.join(image_dir, filename)\n",
        "\n",
        "            if os.path.exists(img_path):\n",
        "                # Setup Plot\n",
        "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6), gridspec_kw={'width_ratios': [1, 1]})\n",
        "\n",
        "                # Show Image\n",
        "                img = mpimg.imread(img_path)\n",
        "                ax1.imshow(img)\n",
        "                ax1.axis('off')\n",
        "                ax1.set_title(f\"Filename: {filename}\", fontsize=12, fontweight='bold')\n",
        "\n",
        "                # Show Mismatches text\n",
        "                text_str = \"MISMATCHED FIELDS:\\n\" + \"-\"*30 + \"\\n\"\n",
        "                for line in lines:\n",
        "                    # formatting for better readability\n",
        "                    # e.g., \"address: '123 fake st' != '123 FAKE ST'\"\n",
        "                    parts = line.split('!=')\n",
        "                    if len(parts) == 2:\n",
        "                        field_part = parts[0].split(':')[0]\n",
        "                        gt_part = parts[0].split(':')[1].strip().strip(\"'\")\n",
        "                        pred_part = parts[1].strip().strip(\"'\")\n",
        "\n",
        "                        text_str += f\"FIELD: {field_part}\\n\"\n",
        "                        text_str += f\"  GT:   {gt_part}\\n\"\n",
        "                        text_str += f\"  PRED: {pred_part}\\n\\n\"\n",
        "                    else:\n",
        "                        text_str += line + \"\\n\"\n",
        "\n",
        "                # Add text to the second subplot\n",
        "                ax2.text(0.05, 0.95, text_str, transform=ax2.transAxes, fontsize=11,\n",
        "                         verticalalignment='top', fontfamily='monospace',\n",
        "                         bbox=dict(boxstyle='round', facecolor='#ffeeee', alpha=0.5))\n",
        "                ax2.axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "                plot_count += 1\n",
        "            else:\n",
        "                print(f\"Warning: Image not found at {img_path}\")\n",
        "\n",
        "# --- RUN IT ---\n",
        "# Adjust paths if your folders are named differently\n",
        "LOG_FILE = '/content/finetuned_eval_log_detailed.txt'\n",
        "IMG_DIR = '/content/ho_img'\n",
        "\n",
        "print(f\"Plotting first 10 failures from {LOG_FILE}...\\n\")\n",
        "plot_mismatches(LOG_FILE, IMG_DIR, max_plots=14)"
      ],
      "metadata": {
        "id": "_MwbUJ5jVlJa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}